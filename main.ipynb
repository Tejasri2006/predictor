{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb56d3c-18ea-43ce-9f0a-334e4a2cc949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Using bucket: c160506a4117400l11163953t1w772864730-sandboxbucket-qaengkzb6snp\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Create or use existing bucket\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"c160506a4117400l11163953t1w772864730-sandboxbucket-qaengkzb6snp\"\n",
    "print(f\"Using bucket: {bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2ef52-4980-4c77-b690-0573e37c94a2",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "This cell sets up the SageMaker and S3 environment for the notebook.\n",
    "\n",
    "**Step-by-step:**\n",
    "- **Imports**:  \n",
    "  - `sagemaker` → to interact with Amazon SageMaker services.  \n",
    "  - `boto3` → AWS SDK for Python, used to interact with S3 and other AWS services.  \n",
    "  - `os` → for handling local file paths and environment variables.  \n",
    "\n",
    "- **SageMaker session & role**:  \n",
    "  - `session = sagemaker.Session()` → creates a SageMaker session for managing interactions.  \n",
    "  - `role = sagemaker.get_execution_role()` → retrieves the IAM role that grants permissions to access AWS resources.\n",
    "\n",
    "- **S3 setup**:  \n",
    "  - `s3 = boto3.client('s3')` → creates an S3 client to upload/download data.  \n",
    "  - `bucket_name` → stores the S3 bucket name where data and models will be saved.  \n",
    "  - `print(...)` → confirms the bucket being used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d168b6e-948e-4c17-a3e6-6e583f1a14f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset S3 path: s3://c160506a4117400l11163953t1w772864730-sandboxbucket-qaengkzb6snp/insurance-rf/data/health_data.csv\n"
     ]
    }
   ],
   "source": [
    "local_data = 'health_data.csv'\n",
    "\n",
    "prefix = 'insurance-rf'\n",
    "data_s3_path = session.upload_data(local_data, bucket=bucket_name, key_prefix=f'{prefix}/data')\n",
    "\n",
    "print(\"Full dataset S3 path:\", data_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fff873-dc10-4395-9e29-9f30e3108148",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "This cell uploads the local dataset to the specified S3 bucket so it can be accessed in AWS.\n",
    "\n",
    "**Step-by-step:**\n",
    "- `local_data = 'health_data.csv'` → specifies the path/name of the local CSV file containing the dataset.\n",
    "- `prefix = 'insurance-rf'` → sets a folder prefix in the S3 bucket to organize related files.\n",
    "- `data_s3_path = session.upload_data(...)` → uploads the local file to S3 under the path `<prefix>/data` inside the chosen bucket.  \n",
    "  - `bucket=bucket_name` → target bucket.  \n",
    "  - `key_prefix=f'{prefix}/data'` → folder structure in S3.\n",
    "- `print(...)` → displays the full S3 path where the dataset is stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1291e84-7311-48ec-8128-abd3133b72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:298: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation R² scores: [0.81049434 0.82718637 0.87472705 0.81790796 0.8486359 ]\n",
      "Mean CV R²: 0.8358\n",
      "Final Train R²: 0.9652\n",
      "Final Test R²: 0.8641\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(f's3://{bucket_name}/{prefix}/data/health_data.csv')\n",
    "\n",
    "\n",
    "target_column = 'charges'  # change if your target is named differently\n",
    "X = df.drop(target_column, axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='r2')\n",
    "print(f\"Cross-validation R² scores: {scores}\")\n",
    "print(f\"Mean CV R²: {np.mean(scores):.4f}\")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f\"Final Train R²: {model.score(X_train, y_train):.4f}\")\n",
    "print(f\"Final Test R²: {model.score(X_test, y_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65d694-a230-442f-aad1-eea0353195df",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "This cell loads the dataset from S3, preprocesses it, builds a Random Forest regression model, evaluates it with cross-validation, and fits the final model.\n",
    "\n",
    "**Step-by-step:**\n",
    "1. **Imports**:  \n",
    "   - `pandas` for data handling.  \n",
    "   - `RandomForestRegressor` for building the prediction model.  \n",
    "   - `train_test_split`, `KFold`, `cross_val_score` for model evaluation.  \n",
    "   - `OneHotEncoder` for encoding categorical variables.  \n",
    "   - `ColumnTransformer` for applying transformations to specific columns.  \n",
    "   - `Pipeline` for combining preprocessing and modeling steps.  \n",
    "   - `numpy` for numerical operations.\n",
    "\n",
    "2. **Load dataset**:  \n",
    "   - `pd.read_csv(f's3://...')` → loads the CSV file directly from the S3 bucket into a DataFrame `df`.\n",
    "\n",
    "3. **Separate features and target**:  \n",
    "   - `target_column = 'charges'` → defines the column to predict.  \n",
    "   - `X = df.drop(target_column, axis=1)` → all input features.  \n",
    "   - `y = df[target_column]` → target variable.\n",
    "\n",
    "4. **Preprocessing setup**:  \n",
    "   - `categorical_cols = X.select_dtypes(include=['object']).columns` → finds columns with string/object data.  \n",
    "   - `preprocessor = ColumnTransformer([...], remainder='passthrough')` → applies One-Hot Encoding to categorical columns and keeps all other columns as-is.\n",
    "\n",
    "5. **Train-test split**:  \n",
    "   - Splits the dataset into 80% training and 20% testing.\n",
    "\n",
    "6. **Pipeline creation**:  \n",
    "   - Combines preprocessing and Random Forest model into a single `Pipeline` for easier training and prediction.\n",
    "\n",
    "7. **Cross-validation**:  \n",
    "   - Uses 5-fold cross-validation (`KFold`) to evaluate model performance on training data.  \n",
    "   - `cross_val_score` returns R² scores for each fold.  \n",
    "   - Prints the mean R² score.\n",
    "\n",
    "8. **Model training & evaluation**:  \n",
    "   - `model.fit(...)` trains the pipeline on training data.  \n",
    "   - Prints final R² scores for both train and test sets to assess performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca52e07-a045-4326-8287-c884f4f8fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted charge: 3884.79\n"
     ]
    }
   ],
   "source": [
    "sample_input = pd.DataFrame([{\n",
    "    'age': 18,\n",
    "    'sex': 'male',\n",
    "    'bmi': 33.77,\n",
    "    'children': 1,\n",
    "    'smoker': 'no',\n",
    "    'region': 'southeast'\n",
    "}])\n",
    "\n",
    "predicted_charge = model.predict(sample_input)\n",
    "print(f\"Predicted charge: {predicted_charge[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bebce-a4ad-47b1-b7bc-63f62fdc7e05",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "This cell creates a sample input for prediction and uses the trained model to estimate insurance charges.\n",
    "\n",
    "**Step-by-step:**\n",
    "1. **Create sample input**:  \n",
    "   - `pd.DataFrame([{...}])` → creates a one-row DataFrame containing the same columns as the training data (`age`, `sex`, `bmi`, `children`, `smoker`, `region`).  \n",
    "   - This simulates a new customer’s data.\n",
    "\n",
    "2. **Make prediction**:  \n",
    "   - `model.predict(sample_input)` → passes the new data through the preprocessing pipeline and the Random Forest model to generate a prediction.  \n",
    "   - The result is a NumPy array with one value.\n",
    "\n",
    "3. **Display result**:  \n",
    "   - `print(...)` → prints the predicted insurance charge, formatted to two decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbae531-a654-4ebe-a6b6-e7a60c0fe200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model packaged & uploaded to: s3://c160506a4117400l11163953t1w772864730-sandboxbucket-qaengkzb6snp/insurance-rf/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import tarfile\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Save model locally as .joblib\n",
    "model_filename = \"rf_insurance_model.joblib\"\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "# Create tar.gz containing the model\n",
    "tar_filename = \"model.tar.gz\"\n",
    "with tarfile.open(tar_filename, \"w:gz\") as tar:\n",
    "    tar.add(model_filename, arcname=model_filename)\n",
    "\n",
    "# Upload to S3 in correct format\n",
    "model_s3_path = f\"s3://{bucket_name}/{prefix}/model/{tar_filename}\"\n",
    "s3.upload_file(tar_filename, bucket_name, f\"{prefix}/model/{tar_filename}\")\n",
    "\n",
    "print(\"Model packaged & uploaded to:\", model_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f7f43-ca2e-43c7-a8ac-cde06cdfc363",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "This cell saves the trained model, packages it for storage, and uploads it to the S3 bucket.\n",
    "\n",
    "**Step-by-step:**\n",
    "1. **Save the model locally**:  \n",
    "   - `joblib.dump(model, model_filename)` → stores the trained pipeline (preprocessing + model) in a `.joblib` file so it can be reused without retraining.\n",
    "\n",
    "2. **Package the model**:  \n",
    "   - Opens a `.tar.gz` archive in write mode.  \n",
    "   - Adds the `.joblib` model file to the archive.  \n",
    "   - The `.tar.gz` format is required by SageMaker for model deployment.\n",
    "\n",
    "3. **Upload to S3**:  \n",
    "   - `s3.upload_file(...)` → uploads the packaged model to the S3 bucket inside the `prefix/model/` folder.  \n",
    "   - `model_s3_path` stores the full S3 path for reference.\n",
    "\n",
    "4. **Confirmation**:  \n",
    "   - Prints the S3 location where the model is saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382342e6-9963-40f8-bfef-db3e947f15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted charge: 4493.04\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "bucket_name = \"c160506a4117400l11163953t1w772864730-sandboxbucket-aci4rul7kn8r\"\n",
    "model_key = \"insurance-rf/model/model.tar.gz\"  # Path in S3\n",
    "\n",
    "\n",
    "local_tar = \"model.tar.gz\"\n",
    "local_model_filename = \"rf_insurance_model.joblib\"\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket_name, model_key, local_tar)\n",
    "\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open(local_tar, \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "model = joblib.load(local_model_filename)\n",
    "\n",
    "\n",
    "sample_input = pd.DataFrame([{\n",
    "    'age': 24,\n",
    "    'sex': 'female',\n",
    "    'bmi': 26.6,\n",
    "    'children': 0,\n",
    "    'smoker': 'no',\n",
    "    'region': 'northeast'\n",
    "}])\n",
    "\n",
    "\n",
    "predicted_charge = model.predict(sample_input)\n",
    "print(f\"Predicted charge: {predicted_charge[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780cfa07-4e67-4a57-9406-e9fe760fe065",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "This cell downloads the trained model from S3, loads it locally, and uses it to make a prediction without deploying to SageMaker.\n",
    "\n",
    "**Step-by-step:**\n",
    "1. **S3 configuration**:  \n",
    "   - `bucket_name` → name of the S3 bucket where the model is stored.  \n",
    "   - `model_key` → path to the packaged `.tar.gz` model file in S3.  \n",
    "   - `local_tar` and `local_model_filename` → local file names for the downloaded archive and extracted model.\n",
    "\n",
    "2. **Download from S3**:  \n",
    "   - Creates an S3 client with `boto3.client('s3')`.  \n",
    "   - `download_file(...)` → retrieves the model archive from S3 and saves it locally as `model.tar.gz`.\n",
    "\n",
    "3. **Extract model**:  \n",
    "   - Opens the `.tar.gz` archive in read mode and extracts its contents (the `.joblib` model file) into the current directory.\n",
    "\n",
    "4. **Load model**:  \n",
    "   - `joblib.load(local_model_filename)` → loads the trained pipeline into memory for inference.\n",
    "\n",
    "5. **Prepare input data**:  \n",
    "   - Creates a one-row DataFrame with the same feature columns as the training dataset to simulate a new prediction request.\n",
    "\n",
    "6. **Make prediction**:  \n",
    "   - `model.predict(sample_input)` → runs the preprocessing and prediction steps on the new data.  \n",
    "   - Prints the predicted insurance charge, formatted to two decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e23d41c-69b9-4225-9ec7-560911273815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<26,>=20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (6.31.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (4.14.1)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (6.0.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (1.47.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Using cached streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, cachetools, pydeck, gitdb, gitpython, altair, streamlit\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [streamlit]/7\u001b[0m [streamlit]\n",
      "\u001b[1A\u001b[2KSuccessfully installed altair-5.5.0 cachetools-6.1.0 gitdb-4.0.12 gitpython-3.1.45 pydeck-0.9.1 smmap-5.0.2 streamlit-1.48.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0a6610-12f0-4540-8ab8-0fbb3775bec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model downloaded from S3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"c160506a4117400l11163953t1w772864730-sandboxbucket-qaengkzb6snp\"\n",
    "key = \"insurance-rf/model/model.tar.gz\"\n",
    "local_file = \"model.tar.gz\"\n",
    "\n",
    "s3.download_file(bucket_name, key, local_file)\n",
    "print(\"✅ Model downloaded from S3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "811c11cc-a4c2-4ad3-9a3e-645332f0b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model extracted\n",
      "Contents: ['rf_insurance_model.joblib']\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Path to your tar.gz file\n",
    "tar_path = \"model.tar.gz\"\n",
    "extract_path = \"model_extracted\"\n",
    "\n",
    "# Create a folder to extract contents\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Extract tar.gz\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "\n",
    "print(\"✅ Model extracted\")\n",
    "\n",
    "# Check extracted files\n",
    "print(\"Contents:\", os.listdir(extract_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "172edd82-4948-4f84-94ee-975a20a06870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Path to your extracted model file\n",
    "model_path = \"model_extracted/rf_insurance_model.joblib\"\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "print(\"✅ Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecce763-0e2a-48b6-9eba-75757e5fcff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
